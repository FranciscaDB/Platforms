{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ba441f-8a69-4d99-bd3f-b8b6da0df3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import signal\n",
    "\n",
    "# Start the usb_cam node as a background process\n",
    "# This provides real-time image data for inference\n",
    "usb_cam_process = subprocess.Popen(\n",
    "    ['ros2', 'run', 'usb_cam', 'usb_cam_node_exe'],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    preexec_fn=os.setsid  # Group the process for easier termination\n",
    ")\n",
    "\n",
    "print(\"usb_cam node started.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14df9c9-26e5-4a9d-83c8-88da0258a73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# Load TensorRT optimized model\n",
    "from torch2trt import TRTModule\n",
    "\n",
    "model_trt = TRTModule()\n",
    "model_trt.load_state_dict(torch.load('best_steering_model_xy_trt.pth'))\n",
    "\n",
    "print(\"Loaded optimized trained weights\")\n",
    "\n",
    "# Define image preprocessing steps to match training format\n",
    "\n",
    "# We have now loaded our model, but there's a slight issue. The format that we trained our model doesn't exactly match the format of the camera. To do that, we need to do some preprocessing. This involves the following steps:\n",
    "\n",
    "# 1. Convert from HWC layout to CHW layout\n",
    "# 2. Normalize using same parameters as we did during training (our camera provides values in [0, 255] range and training loaded images in [0, 1] range so we need to scale by 255.0\n",
    "# 3. Transfer the data from CPU memory to GPU memory\n",
    "# 4. Add a batch dimension\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "\n",
    "mean = torch.Tensor([0.485, 0.456, 0.406]).cuda().half()\n",
    "std = torch.Tensor([0.229, 0.224, 0.225]).cuda().half()\n",
    "\n",
    "def preprocess(image):\n",
    "    image = PIL.Image.fromarray(image).resize((224, 224))\n",
    "    image = transforms.functional.to_tensor(image).to(device).half()\n",
    "    image.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
    "    return image[None, ...]\n",
    "\n",
    "print(\"Created the Pre-Processing Function\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916cb053-3dc2-4470-93cd-9d96000504c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# PID and inference sliders for control and feedback\n",
    "speed_gain_slider = widgets.FloatSlider(min=0.0, max=1.0, step=0.01, description='speed gain')\n",
    "steering_gain_slider = widgets.FloatSlider(min=0.0, max=1.0, step=0.01, value=0.2, description='steering gain')\n",
    "steering_dgain_slider = widgets.FloatSlider(min=0.0, max=0.5, step=0.001, value=0.0, description='steering kd')\n",
    "steering_bias_slider = widgets.FloatSlider(min=-0.3, max=0.3, step=0.01, value=0.0, description='steering bias')\n",
    "\n",
    "display(speed_gain_slider, steering_gain_slider, steering_dgain_slider, steering_bias_slider)\n",
    "\n",
    "# Sliders for real-time predictions\n",
    "x_slider = widgets.FloatSlider(min=-1.0, max=1.0, description='x')\n",
    "y_slider = widgets.FloatSlider(min=0, max=1.0, orientation='vertical', description='y')\n",
    "steering_slider = widgets.FloatSlider(min=-1.0, max=1.0, description='steering')\n",
    "\n",
    "display(x_slider, y_slider, steering_slider)\n",
    "\n",
    "# Image display widget\n",
    "image_widget = widgets.Image(format='jpeg')\n",
    "display(image_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a3dd90-e1c3-4abf-abd6-bd4d3ef1b507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rclpy\n",
    "from rclpy.node import Node\n",
    "from sensor_msgs.msg import Image as ROSImage\n",
    "from cv_bridge import CvBridge\n",
    "import cv2\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "from threading import Thread\n",
    "import numpy as np\n",
    "import time\n",
    "from geometry_msgs.msg import Twist\n",
    "\n",
    "def bgr8_to_jpeg(value):\n",
    "    return cv2.imencode('.jpg', value)[1].tobytes()\n",
    "    \n",
    "angle = 0.0\n",
    "angle_last = 0.0\n",
    "bridge = CvBridge()\n",
    "\n",
    "# This node listens to the camera feed, runs inference,\n",
    "# calculates steering angle using a basic PID controller,\n",
    "# and publishes the result as a Twist message.\n",
    "class ImageSubscriber(Node):\n",
    "    def __init__(self):\n",
    "        super().__init__('image_subscriber')\n",
    "        self.subscription = self.create_subscription(\n",
    "            ROSImage,\n",
    "            '/image_raw',\n",
    "            self.listener_callback,\n",
    "            10)\n",
    "        \n",
    "        self.twist_publisher = self.create_publisher(Twist, 'JetBot1/target', 10)\n",
    "    \n",
    "    def listener_callback(self, msg):\n",
    "        global angle, angle_last\n",
    "        frame = bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n",
    "        \n",
    "        # Display input frame\n",
    "        image_widget.value = bgr8_to_jpeg(frame)\n",
    "        \n",
    "        # Run inference\n",
    "        xy = model_trt(preprocess(frame)).detach().float().cpu().numpy().flatten()\n",
    "        x = xy[0]\n",
    "        y = (0.5 - xy[1]) / 2.0\n",
    "        \n",
    "        # Update sliders with predictions\n",
    "        x_slider.value = x\n",
    "        y_slider.value = y\n",
    "\n",
    "        # Compute steering using a proportional-derivative controller\n",
    "        angle = np.arctan2(x, y)\n",
    "        pid = angle * steering_gain_slider.value + (angle - angle_last) * steering_dgain_slider.value\n",
    "        angle_last = angle\n",
    "        steering_slider.value = pid + steering_bias_slider.value\n",
    "        \n",
    "        # Publish Twist message\n",
    "        twist_msg = Twist()\n",
    "        twist_msg.linear.x = speed_gain_slider.value     \n",
    "        twist_msg.angular.z = steering_slider.value\n",
    "        self.twist_publisher.publish(twist_msg)\n",
    "\n",
    "# Run the ROS 2 node in a background thread\n",
    "def start_ros_node():\n",
    "    if not rclpy.ok():\n",
    "        rclpy.init()\n",
    "    node = ImageSubscriber()\n",
    "    rclpy.spin(node)\n",
    "    node.destroy_node()\n",
    "    rclpy.shutdown()\n",
    "\n",
    "ros_thread = Thread(target=start_ros_node, daemon=True)\n",
    "ros_thread.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a28d94-5be7-4c58-8151-2f45be5983fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the camera node process when done\n",
    "os.killpg(os.getpgid(usb_cam_process.pid), signal.SIGINT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
